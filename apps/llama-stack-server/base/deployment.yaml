kind: Deployment
apiVersion: apps/v1
metadata:
  name: llama-stack-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-stack-server
  template:
    metadata:
      labels:
        app: llama-stack-server
    spec:
      volumes:
        - name: run-config-volume
          configMap:
            name: run-config
            defaultMode: 420
        - name: llama-persist
          persistentVolumeClaim:
            claimName: llama-persist
        - name: cache
          emptyDir: {}
        - name: pythain
          emptyDir: {}
      containers:
        - resources: {}
          terminationMessagePath: /dev/termination-log
          name: llama-stack-server
          env:
            - name: VLLM_URL
              value: 'https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1'
            - name: INFERENCE_MODEL
              value: llama-4-scout-17b-16e-w4a16
            - name: VLLM_API_TOKEN
              value: c75b821cfdf99b89f109dfea23f1b33e
            - name: VLLM_TLS_VERIFY
              value: 'false'
            - name: CUSTOM_TIKTOKEN_CACHE_DIR
              value: /app/cache
            - name: LLAMA_STACK_LOGGING
              value: all=debug
            - name: MCP_SERVER_OCP_URL
              value: http://ocp-agent-service.rh-proposal-ai.svc.cluster.local:7860/gradio_api/mcp/sse
          ports:
            - containerPort: 8321
              protocol: TCP
          imagePullPolicy: Always
          volumeMounts:
            - name: pythain
              mountPath: /pythainlp-data
            - name: run-config-volume
              mountPath: /app-config
            - name: llama-persist
              mountPath: /.llama
            - name: cache
              mountPath: /.cache
          terminationMessagePolicy: File
          image: 'quay.io/lcoronad/llama-stack-remote-vllm:dev-1.0.0'
          args:
            - '--config'
            - /app-config/run.yaml
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      securityContext: {}
      schedulerName: default-scheduler
      serviceAccountName: llama-stack-server
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
